---
layout: post
title: "Kubo Oscillator"
---

In this post we introduce the *Kubo oscillator* as a representative example of what we shall term *multiplicative* latent Gaussian Process force models. The evolution of the Kubo Oscillator is decribed by the differential equation

$$
\begin{bmatrix} \dot{x}_1 \\ \dot{x_2} \end{bmatrix}
= \left( \mathbf{B} + \begin{bmatrix} 0 & -\epsilon(t) \\ \epsilon(t) & 0 \end{bmatrix} \right)\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \tag{1}
$$

where $ \epsilon(t) $ is a *smooth* Gaussian process, and therefore we may intepret (1) as a conventional ordinary differential equation. So long as the matrix $$\mathbf{B}$$.

## Inferring the model parameters
Given a set of observations $\mathbf{X}$ which we believe to have been generated by a model of the form (1) with unknown values of the $\mathbf{A}, \mathbf{B}, \epsilon(t) $ we turn our attention to how inference may proceed. Importantly here when $\mathbf{B} = 0$ (1) one may be solved to give

$$
\begin{align*}
\begin{bmatrix} x_{n+1} \\ y_{n+1} \end{bmatrix} &= \exp\left(
\begin{bmatrix} 0 & \int_{t_n}^{t_{n+1}} \epsilon(t)dt \\
-\int_{t_n}^{t_{n+1}} \epsilon(t) dt  & 0
\end{bmatrix} \right)\begin{bmatrix} x_n \\ y_n \end{bmatrix}, \\
&= \begin{bmatrix}
\cos \mathcal{E}(t_n,t_{n+1} ) & - \sin \mathcal{E}(t_n,t_{n+1} ) \\
\sin \mathcal{E}(t_n,t_{n+1} ) & \cos \mathcal{E}(t_n,t_{n+1} )
\end{bmatrix}\begin{bmatrix} x_n \\ y_n \end{bmatrix}, \\
\end{align*}
$$

where $\mathcal{E}(t_{n}, t_{n+1})$ is the integrated Gaussian process, which is itself a Gaussian process. It follows that if $\mathbf{\theta} = \theta_1, \ldots, \theta_n$ is a sequence of angles $\theta \in [0,2\pi]$ such that $R(\theta_i) x_{i} = x_{i+1}$ then the posterior of $\mathcal{E}_i$ is given by

$$
p(\mathcal{E}_1, \ldots, \mathcal{E}_n | \mathbf{X} ) \propto p(\mathcal{E}_1 =  \theta_1 + 2\pi k_1, \ldots ), \qquad k_i \in \mathbb{Z}
$$

it follows that the Kubo oscillator can be solved exactly.

## Comparison with gradient matching
A gradient matching approach would proceed by placing a gaussian process prior on the observed trajectories $\mathbf{x}$, in this instance however such a prior assumption is hard to justify - gradient matching therefore relies on the model data to correct for the the lost mathematical structure in adopting the significantly simplified prior.

In the case of the model we are currently considering the discussion above shows that we can solve for the approporiate posterior so that exact inference can be achieved, on the otherhand we would like to have sufficiently flexible models to deal with situations in which this is no longer necessarily so, but we have reason to believe there is enough mathematical structure - such as a tendency for the data (not the data but the true trajectories) to concentrate on proper submanifolds of the embedding space - that we would like to develop models which capture some element of this structure while still allowing for practical inference.

## Missing data problem
And common problem in time series models is the resampling of a new point $\mathbf{x}_{i} $ conditional on points $\mathbf{x_{i-1}} $ and $\mathbf{x}_{i+1} $ say.